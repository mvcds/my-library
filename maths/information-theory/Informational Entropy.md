---
aliases:
  - Entropy
  - Information
---
A key measure in [[Information Theory]], it quantifies the amount of [[Uncertainty]] involved in the [[my-library/philosofy/Value|Value]] of a [[randomness|random]] value.

For example a fair coin flip provides less information (less entropy) than a roll of a fair six-face die.
