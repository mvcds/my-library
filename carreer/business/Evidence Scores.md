https://medium.com/@itamargilad/evidence-based-scoring-a-systematic-way-to-know-if-you-have-a-good-idea-44d39e166abf

Finding and building the next big idea is the holy grail of any tech company. This score is a way to tell if an idea is any good before making the big investment.

1. We look at what supporting evidence the idea has (e.g. market research, surveys, successful A/B experiment)
2. Based on the evidence we give the idea a confidence score (details below) that indicates how sure we are now that this idea is a winner.
3. If confidence is high we can go ahead with building and launching the idea.
4. If confidence is not high we can decide to test the idea further (for example run a user study) to produce more evidence, to de-prioritize the idea in favor of other ideas, or to kill it altogether.
5. Our level of investment in the idea should always stay proportional to our level of confidence in it. That’s right, no more big CEO pet projects that have no supporting [data] — ideas have to live and die on their on merit.

# Quick reference table

![[evidence-score-1.png]]

Earlier validation steps rely mostly on subjective opinions, projections and interpretations by individuals and groups.

As you move forward in testing probability drops sharply, so an idea that survives the later stages is orders of magnitude more likely to be good than one that was subjected just to early tests.

As you go through the steps of validation you also learn more: about users, the market and how well your idea fits both. This gives you a chance not just to validate your idea, but also to improve it, usually long before it is launched

> In fact most great ideas are not born great (no eureka moment), they become good through the process of testing, learning and refinement/pivots.

#quote

# Score your idea

Obviously you don’t have to run every idea though every test in the list. Certainly for big ideas that require substantial investment it is good idea to to go through the test groups and perform at least a couple of things in each. Some data-driven companies like Netflix and Booking.com skip the early conviction/thematic-support/pitch phases altogether and empower their teams to A/B test almost anything. Other companies rely much more on qualitative research. Either way, doing more, especially of the mid and late tests, is very important.

For each "evidence", add the equivalent points

Stage | Points
- | -
Self Conviction | 0
Crafted Pitch | 0.1
Thematic support | 0.2
Other’s Opinions | 1
High level plans and estimates | 5
Anecdotal evidence | 20
Market data | 100
User/Customer evidence | 500
Test results | 2,000
Early launch results | 10,000 
Late launch results | Good Idea Badge!

Check against the results

- 0-5 - Very low confidence: this idea is almost entirely based on opinions and theory rather than data and fact.
- 6-100 - Low confidence: You’ll need to get more real data and listen to many more potential users to grow in confidence in order to avoid the [[Optimism Bias]]
- 101–500 points — Medium-Low confidence: you’re working with small samples that can easily be skewed or misinterpreted. Still you’ve started learning and improving (or abandoning) your idea based on objective evidence — an important milestone
- 501–2000 — Medium-High confidence: using quantitative and qualitative research to test the main hypothesis of your idea might not be enough to confirm the idea is worth launching
- 2001–10,000 points — high confidence: If you got good results here you should feel fairly confident you have a winner — launch it!. Still, sometimes statistics or an unexpected snafu can play a nasty trick on us, so be optimistic but watchful
- 10,000+ points — very high confidence — Congratulation — the idea is now a launched! Still, in the weeks and months after launch there’s still some level of uncertainty about how the product is performing as well as novelty effects. This is not the time to let your guard down — keep processing the data and interviewing people to make sure you’re not missing anything.

# Self Conviction

- You think this is a good idea
- You perceive yourself as a smart and capable product person
- You feel your product ideas are usually pretty good (sometimes even brilliant).

# Crafted Pitch 

- You can clearly articulate why this is going to help the users and the company
- You have a short and elegant elevator pitch
- You have a polished pitch deck

# Thematic support 

- The idea matches your company’s or investor’s vision or strategy
- The idea is inline with a current industry trend/buzzword.
- Outside research reports could be interpreted to support the idea.
- The idea seems to be aligned with macro trends in economy/society
- The idea fits a current product development or design methodology/philosophy

# Other’s Opinions

- The team thinks it’s a good idea
- Management thinks it’s a good idea
- An external expert thinks it’s a good idea
- An investor thinks it’s a good idea
- Tech press thinks it’s a good idea

# High level plans and estimates

- Back-of-the-envelope calculations (e.g. funnel numbers) show good potential
- Engineering and UX feel the idea is feasible
- Effort estimates from the team converge into a reasonable project timeframe
- A draft business model or business plan suggest good business-viability.

# Anecdotal evidence

- 2–3 data points in your product data support the idea
- Sales say it’s a top request
- You’ve spoken to 1–3 customers who seem interested
- One competitor has it implemented

# Market data

- Surveys you conducted or commissioned clearly shows user/customer support for the idea
- Smoke tests (for example a “fake door” ad campaign) get strong positive results (for example high ad CTR).
- Multiple/all competitors have it.

# User/Customer evidence

- Significant amount of product data across multiple months show support for this idea
- A customer support/success report surfaces this as a top request by multiple customers
- You’ve interviewed 20+ potential users/customers and 70%+ of them say they would use this and/or be willing to pay for it.
- You ran a usability study with 10+ users and 80%+ of them understood the idea, were able to use it, and said they would use it.
- You ran a successful small scale concierge MVP study with 1–5 users.

# Test results

- You launched 2–4 week longitudinal study and 70%+ of participants kept using the product and were interested to buy or continue using at the end of the test.
- You’ve built an MVP and had 50+ active users/customers that at willing to use/purchase and are interested.
- You launched an alpha or beta version to 20+ early tester customers.
- You’ve ran an A/B test and the experiment group shows the key metrics you outlined, and no decline in other key metrics. All results are statistically significant with p-value of 5% of below.

# Early launch results

1 month after launch:
- The feature gets repeat usage from a significant % of users
- Compared to a 5% holdback group the user group that got the product change demonstrate better product stats (e.g. better retention, time spent)
- Customer support/success reports shows very positive feedback from users
- Business metrics have improved since the launch

# Late launch results

12–24 months after the launch you have:
- Positive user feedback outweighs negative 10-to-1
- Metrics show strong repeat usage of the new feature
- Business metrics went up and stayed there