A self-[[Attention Mechanism]] used in [[Natural Language Processing|NLP]] to not process [[text]] in a sequential order, as opposed to [[Recurrent Neural Network|RNN]] and [[Convolutional Neural Network|CNN]], thus being more efficient due to [[Parallelism]] and more efficient.

It associates [[Word|Words]] with positions in a way that it understands how they relate to the global context, which allows them to be rearranged and preserve their "local" meaning.

#learn read "Attention Is All You Need"
