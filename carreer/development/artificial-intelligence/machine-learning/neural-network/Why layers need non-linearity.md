![[why-layers-need-non-linearity.png]]

A [[Layer]] has to have a [[Activation Function|Non-linear Function]] because [[Maths|mathematically]] this is what makes stacking layers in a [[Neural Network]] possible - otherwise we could not make them deep and they'd be only a simple mathematical function

That is, two consecutive linearities can be reduced to a single one