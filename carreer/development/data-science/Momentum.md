A correction over gradient descent algorithms where you consider previous [[my-library/carreer/development/Value|values]] to fix new ones in order to not get stuck into local minimum.

Momentum can be adjusted as its an [[Parameters vs Hyperparameters in Neural Networks|hyperparameter]] during [[Optimization function]]
